{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c74418c-855d-429c-873a-8cd230ad05aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\subbu\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import webcolors\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597770e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load and process an image\n",
    "def load_image(image_path, img_size=(64, 64)):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, img_size)\n",
    "    return img\n",
    "\n",
    "# Function to reshape image for KMeans\n",
    "def reshape_image(img):\n",
    "    return img.reshape((-1, 3))\n",
    "\n",
    "# Function to apply KMeans and return dominant colors\n",
    "def extract_dominant_colors(img, num_colors=3):\n",
    "    reshaped_img = reshape_image(img)\n",
    "    kmeans = KMeans(n_clusters=num_colors)\n",
    "    kmeans.fit(reshaped_img)\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "# Function to create color labels\n",
    "def create_color_labels(image_paths, num_colors=3):\n",
    "    color_labels = defaultdict(int)\n",
    "    image_labels = []\n",
    "    for path in image_paths:\n",
    "        img = load_image(path)\n",
    "        colors = extract_dominant_colors(img, num_colors)\n",
    "        colors_tuple = tuple(map(tuple, colors))\n",
    "        if colors_tuple not in color_labels:\n",
    "            color_labels[colors_tuple] = len(color_labels)\n",
    "        image_labels.append(color_labels[colors_tuple])\n",
    "\n",
    "    return image_labels, color_labels\n",
    "\n",
    "# Function to prepare dataset\n",
    "def prepare_dataset(image_paths, image_labels, img_size=(64, 64)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for path, label in zip(image_paths, image_labels):\n",
    "        img = load_image(path, img_size)\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "    images = np.array(images) / 255.0  # Normalize the images\n",
    "    labels = tf.keras.utils.to_categorical(labels)  # One-hot encoding of labels\n",
    "\n",
    "    return train_test_split(images, labels, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ff418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\subbu\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\subbu\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\subbu\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\subbu\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\subbu\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - 2s 149ms/step - loss: 5.4336 - accuracy: 0.0000e+00 - val_loss: 5.3710 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 5.2578 - accuracy: 0.0000e+00 - val_loss: 5.3406 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 5.2278 - accuracy: 0.0234 - val_loss: 5.3764 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 5.1524 - accuracy: 0.0234 - val_loss: 5.5633 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 5.0198 - accuracy: 0.0312 - val_loss: 5.9255 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 4.8776 - accuracy: 0.0625 - val_loss: 6.0921 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 4.6770 - accuracy: 0.0859 - val_loss: 6.3102 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 4.4325 - accuracy: 0.1562 - val_loss: 6.5984 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 4.0851 - accuracy: 0.2109 - val_loss: 7.2179 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 3.8889 - accuracy: 0.2188 - val_loss: 7.8544 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 3.4301 - accuracy: 0.2969 - val_loss: 8.2276 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 2.9407 - accuracy: 0.4375 - val_loss: 9.8755 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 2.4689 - accuracy: 0.5078 - val_loss: 10.6631 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 1.9610 - accuracy: 0.6562 - val_loss: 12.0337 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 1.4977 - accuracy: 0.7578 - val_loss: 13.8433 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.9979 - accuracy: 0.8906 - val_loss: 15.7812 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.7202 - accuracy: 0.9219 - val_loss: 18.5496 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.4512 - accuracy: 0.9766 - val_loss: 20.6611 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.2649 - accuracy: 0.9844 - val_loss: 22.6743 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.1532 - accuracy: 1.0000 - val_loss: 24.9466 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# CNN Model Building\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "import os\n",
    "def get_image_paths(folder_path, valid_extensions=(\".jpg\", \".jpeg\", \".png\")):\n",
    "    image_paths = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(valid_extensions):\n",
    "            image_paths.append(os.path.join(folder_path, filename))\n",
    "    return image_paths\n",
    "# Replace 'your_image_folder_path' with the path to your folder containing images\n",
    "folder_path = 'small_img'  # Update this with your folder path\n",
    "image_paths = get_image_paths(folder_path)\n",
    "\n",
    "# The rest of your code for creating labels and preparing the dataset\n",
    "image_labels, _ = create_color_labels(image_paths)\n",
    "X_train, X_test, y_train, y_test = prepare_dataset(image_paths, image_labels)\n",
    "\n",
    "\n",
    "\n",
    "# # Replace this with the actual image paths\n",
    "# image_paths = ['upload/1 (13).jpg']  # Update this list with your image paths\n",
    "# image_labels, _ = create_color_labels(image_paths)\n",
    "# X_train, X_test, y_train, y_test = prepare_dataset(image_paths, image_labels)\n",
    "\n",
    "# Build and train the CNN\n",
    "num_classes = y_train.shape[1]\n",
    "model = build_cnn(X_train.shape[1:], num_classes)\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433fa731-3a1e-4117-b072-0590261986ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Access the history to get training accuracy\n",
    "training_accuracy = history.history['accuracy']\n",
    "print(\"Training Accuracy: \", training_accuracy[-1])\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c374727",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    'black': ['Powerful', 'sophisticated', 'edgy'],\n",
    "    'firebrick': ['Passionate', 'aggressive', 'important'],\n",
    "    'gold': ['Opulent', 'traditional', 'prestigious'],\n",
    "    'silver': ['Sleek', 'graceful', 'futuristic'],\n",
    "    'turquoise': ['Refreshing', 'tranquil', 'creative'],\n",
    "    'lavender': ['Delicate', 'graceful', 'nostalgic'],\n",
    "    'beige': ['Simplistic', 'dependable', 'conservative'],\n",
    "    'palevioletred': ['Dynamic', 'bold', 'passionate'],\n",
    "    'cornflowerblue': ['Imaginative', 'spirited', 'unique'],\n",
    "    'darkslategray': ['Natural', 'peaceful', 'enduring'],\n",
    "    'rosybrown': ['Warm', 'inviting', 'vibrant'],\n",
    "    'darkolivegreen': ['Professional', 'reliable', 'authoritative'],\n",
    "    'olivedrab': ['Energetic', 'lively', 'fresh'],\n",
    "    'lightgray': ['Solid', 'professional', 'mature'],\n",
    "    'steelblue': ['Fresh', 'cool', 'youthful'],\n",
    "    'darkgray': ['Deep', 'wise', 'thoughtful'],\n",
    "    'dimgray': ['Lush', 'vibrant', 'sophisticated'],\n",
    "    'cadetblue': ['Earthy', 'warm', 'enduring'],\n",
    "    'gray': ['Soft', 'friendly', 'approachable'],\n",
    "    'slategray': ['Refreshing', 'serene', 'youthful']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f25b6bf-bde6-4b61-a864-238aaff88f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n",
      "Dominant Hex Color: #d9d8d4\n",
      "Predicted Color : lightgray\n",
      "Emotions: ['Solid', 'professional', 'mature']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import webcolors\n",
    "\n",
    "def closest_color(requested_color):\n",
    "    min_colors = {}\n",
    "    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "        rd = (r_c - requested_color[0]) ** 2\n",
    "        gd = (g_c - requested_color[1]) ** 2\n",
    "        bd = (b_c - requested_color[2]) ** 2\n",
    "        min_colors[(rd + gd + bd)] = name\n",
    "    return min_colors[min(min_colors.keys())]\n",
    "\n",
    "def get_color_name(hex_value):\n",
    "    try:\n",
    "        color_name = webcolors.hex_to_name(hex_value)\n",
    "    except ValueError:\n",
    "        rgb_value = webcolors.hex_to_rgb(hex_value)\n",
    "        color_name = closest_color(rgb_value)\n",
    "    return color_name\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, img_size=(64, 64)):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize and normalize the image\n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    return np.expand_dims(img, axis=0)\n",
    "\n",
    "def find_dominant_color(image, k=1):\n",
    "    # Reshape the image and apply KMeans clustering\n",
    "    reshaped_img = image.reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(reshaped_img)\n",
    "\n",
    "    # Get the dominant color\n",
    "    dominant_color = kmeans.cluster_centers_[0].astype(int)\n",
    "\n",
    "    # Convert the dominant color to hex\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(dominant_color[0], dominant_color[1], dominant_color[2])\n",
    "\n",
    "# Load your trained model\n",
    "loaded_model = load_model('model.h5')\n",
    "\n",
    "# Path to the new image you want to predict on\n",
    "new_image_path = 'upload/1 (10).jpg'\n",
    "\n",
    "# Load and preprocess the image\n",
    "preprocessed_image = load_and_preprocess_image(new_image_path)\n",
    "\n",
    "# Predict the class of the image\n",
    "predictions = loaded_model.predict(preprocessed_image)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Load the image again in its original size for color extraction\n",
    "original_image = cv2.imread(new_image_path)\n",
    "original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Find the dominant color\n",
    "dominant_hex_color = find_dominant_color(original_image)\n",
    "color=get_color_name(dominant_hex_color)\n",
    "main_emotion = emotions[color]\n",
    "# Output\n",
    "# print(\"Predicted Class:\", predicted_class[0])\n",
    "print(\"Dominant Hex Color:\", dominant_hex_color)\n",
    "print(\"Predicted Color :\",color) \n",
    "print(\"Emotions:\",main_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6bd020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n",
      "#201e1a\n",
      "black\n",
      "['Powerful', 'sophisticated', 'edgy']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming the other functions (closest_color, get_color_name, find_dominant_color) are defined as before\n",
    "\n",
    "def process_video(video_path, model_path, img_size=(64, 64)):\n",
    "    # Load the trained model\n",
    "    loaded_model = load_model(model_path)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess the frame (resize, normalize)\n",
    "        frame_processed = cv2.resize(frame, img_size)\n",
    "        frame_processed = frame_processed / 255.0\n",
    "        frame_processed = np.expand_dims(frame_processed, axis=0)\n",
    "\n",
    "        # Predict the class for the current frame\n",
    "        predictions = loaded_model.predict(frame_processed)\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Convert frame to RGB and find the dominant color\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        dominant_hex_color = find_dominant_color(frame_rgb)\n",
    "        color = get_color_name(dominant_hex_color)\n",
    "        main_emotion = emotions[color]\n",
    "        return dominant_hex_color,color,main_emotion\n",
    "        # Process the results as needed (e.g., display, store, aggregate)\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "# Example usage\n",
    "video_path = 'upload/v2.mp4'\n",
    "model_path = 'model.h5'\n",
    "dominant_hex_color,color,main_emotion = process_video(video_path, model_path)\n",
    "print(dominant_hex_color)\n",
    "print(color)\n",
    "print(main_emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c0eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff29eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

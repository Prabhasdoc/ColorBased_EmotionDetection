{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c74418c-855d-429c-873a-8cd230ad05aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwebcolors\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import webcolors\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597770e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load and process an image\n",
    "def load_image(image_path, img_size=(64, 64)):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, img_size)\n",
    "    return img\n",
    "\n",
    "# Function to reshape image for KMeans\n",
    "def reshape_image(img):\n",
    "    return img.reshape((-1, 3))\n",
    "\n",
    "# Function to apply KMeans and return dominant colors\n",
    "def extract_dominant_colors(img, num_colors=3):\n",
    "    reshaped_img = reshape_image(img)\n",
    "    kmeans = KMeans(n_clusters=num_colors)\n",
    "    kmeans.fit(reshaped_img)\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "# Function to create color labels\n",
    "def create_color_labels(image_paths, num_colors=3):\n",
    "    color_labels = defaultdict(int)\n",
    "    image_labels = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        img = load_image(path)\n",
    "        colors = extract_dominant_colors(img, num_colors)\n",
    "        colors_tuple = tuple(map(tuple, colors))\n",
    "        if colors_tuple not in color_labels:\n",
    "            color_labels[colors_tuple] = len(color_labels)\n",
    "        image_labels.append(color_labels[colors_tuple])\n",
    "\n",
    "    return image_labels, color_labels\n",
    "\n",
    "# Function to prepare dataset\n",
    "def prepare_dataset(image_paths, image_labels, img_size=(64, 64)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for path, label in zip(image_paths, image_labels):\n",
    "        img = load_image(path, img_size)\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "    images = np.array(images) / 255.0  # Normalize the images\n",
    "    labels = tf.keras.utils.to_categorical(labels)  # One-hot encoding of labels\n",
    "\n",
    "    return train_test_split(images, labels, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ff418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - 3s 350ms/step - loss: 5.3631 - accuracy: 0.0000e+00 - val_loss: 5.4165 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 1s 245ms/step - loss: 5.2423 - accuracy: 0.0078 - val_loss: 5.3986 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 1s 244ms/step - loss: 5.1641 - accuracy: 0.0156 - val_loss: 5.5639 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 1s 249ms/step - loss: 5.0324 - accuracy: 0.0234 - val_loss: 6.0812 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 1s 319ms/step - loss: 4.8506 - accuracy: 0.0469 - val_loss: 6.2724 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 1s 241ms/step - loss: 4.5966 - accuracy: 0.1484 - val_loss: 6.4798 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 1s 274ms/step - loss: 4.2640 - accuracy: 0.2109 - val_loss: 7.2746 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 1s 274ms/step - loss: 3.8843 - accuracy: 0.2266 - val_loss: 7.7600 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 1s 274ms/step - loss: 3.4241 - accuracy: 0.3359 - val_loss: 8.9180 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 1s 349ms/step - loss: 2.8929 - accuracy: 0.5000 - val_loss: 9.9545 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 1s 270ms/step - loss: 2.3333 - accuracy: 0.6094 - val_loss: 11.6132 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 1s 340ms/step - loss: 1.7423 - accuracy: 0.7500 - val_loss: 13.5197 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 1s 267ms/step - loss: 1.2796 - accuracy: 0.8281 - val_loss: 14.6375 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 1s 286ms/step - loss: 0.8364 - accuracy: 0.8828 - val_loss: 16.8223 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 1s 261ms/step - loss: 0.5186 - accuracy: 0.9141 - val_loss: 18.7067 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 1s 265ms/step - loss: 0.3512 - accuracy: 0.9688 - val_loss: 20.1358 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 1s 254ms/step - loss: 0.1792 - accuracy: 0.9922 - val_loss: 22.6942 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 1s 244ms/step - loss: 0.0871 - accuracy: 1.0000 - val_loss: 24.3125 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 1s 317ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 26.6113 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 1s 242ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 27.7627 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# CNN Model Building\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "import os\n",
    "def get_image_paths(folder_path, valid_extensions=(\".jpg\", \".jpeg\", \".png\")):\n",
    "    image_paths = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(valid_extensions):\n",
    "            image_paths.append(os.path.join(folder_path, filename))\n",
    "    return image_paths\n",
    "# Replace 'your_image_folder_path' with the path to your folder containing images\n",
    "folder_path = 'small_img'  # Update this with your folder path\n",
    "image_paths = get_image_paths(folder_path)\n",
    "\n",
    "# The rest of your code for creating labels and preparing the dataset\n",
    "image_labels, _ = create_color_labels(image_paths)\n",
    "X_train, X_test, y_train, y_test = prepare_dataset(image_paths, image_labels)\n",
    "\n",
    "\n",
    "\n",
    "# # Replace this with the actual image paths\n",
    "# image_paths = ['upload/1 (13).jpg']  # Update this list with your image paths\n",
    "# image_labels, _ = create_color_labels(image_paths)\n",
    "# X_train, X_test, y_train, y_test = prepare_dataset(image_paths, image_labels)\n",
    "\n",
    "# Build and train the CNN\n",
    "num_classes = y_train.shape[1]\n",
    "model = build_cnn(X_train.shape[1:], num_classes)\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433fa731-3a1e-4117-b072-0590261986ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Access the history to get training accuracy\n",
    "training_accuracy = history.history['accuracy']\n",
    "print(\"Training Accuracy: \", training_accuracy[-1])\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c374727",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    'black': ['Powerful', 'sophisticated', 'edgy'],\n",
    "    'firebrick': ['Passionate', 'aggressive', 'important'],\n",
    "    'gold': ['Opulent', 'traditional', 'prestigious'],\n",
    "    'silver': ['Sleek', 'graceful', 'futuristic'],\n",
    "    'turquoise': ['Refreshing', 'tranquil', 'creative'],\n",
    "    'lavender': ['Delicate', 'graceful', 'nostalgic'],\n",
    "    'beige': ['Simplistic', 'dependable', 'conservative'],\n",
    "    'palevioletred': ['Dynamic', 'bold', 'passionate'],\n",
    "    'cornflowerblue': ['Imaginative', 'spirited', 'unique'],\n",
    "    'darkslategray': ['Natural', 'peaceful', 'enduring'],\n",
    "    'rosybrown': ['Warm', 'inviting', 'vibrant'],\n",
    "    'darkolivegreen': ['Professional', 'reliable', 'authoritative'],\n",
    "    'olivedrab': ['Energetic', 'lively', 'fresh'],\n",
    "    'lightgray': ['Solid', 'professional', 'mature'],\n",
    "    'steelblue': ['Fresh', 'cool', 'youthful'],\n",
    "    'darkgray': ['Deep', 'wise', 'thoughtful'],\n",
    "    'dimgray': ['Lush', 'vibrant', 'sophisticated'],\n",
    "    'cadetblue': ['Earthy', 'warm', 'enduring'],\n",
    "    'gray': ['Soft', 'friendly', 'approachable'],\n",
    "    'slategray': ['Refreshing', 'serene', 'youthful']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f25b6bf-bde6-4b61-a864-238aaff88f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n",
      "Dominant Hex Color: #d9d8d4\n",
      "Predicted Color : lightgray\n",
      "Emotions: ['Solid', 'professional', 'mature']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import webcolors\n",
    "\n",
    "def closest_color(requested_color):\n",
    "    min_colors = {}\n",
    "    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "        rd = (r_c - requested_color[0]) ** 2\n",
    "        gd = (g_c - requested_color[1]) ** 2\n",
    "        bd = (b_c - requested_color[2]) ** 2\n",
    "        min_colors[(rd + gd + bd)] = name\n",
    "    return min_colors[min(min_colors.keys())]\n",
    "\n",
    "def get_color_name(hex_value):\n",
    "    try:\n",
    "        color_name = webcolors.hex_to_name(hex_value)\n",
    "    except ValueError:\n",
    "        rgb_value = webcolors.hex_to_rgb(hex_value)\n",
    "        color_name = closest_color(rgb_value)\n",
    "    return color_name\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, img_size=(64, 64)):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize and normalize the image\n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    return np.expand_dims(img, axis=0)\n",
    "\n",
    "def find_dominant_color(image, k=1):\n",
    "    # Reshape the image and apply KMeans clustering\n",
    "    reshaped_img = image.reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(reshaped_img)\n",
    "\n",
    "    # Get the dominant color\n",
    "    dominant_color = kmeans.cluster_centers_[0].astype(int)\n",
    "\n",
    "    # Convert the dominant color to hex\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(dominant_color[0], dominant_color[1], dominant_color[2])\n",
    "\n",
    "# Load your trained model\n",
    "loaded_model = load_model('model.h5')\n",
    "\n",
    "# Path to the new image you want to predict on\n",
    "new_image_path = 'upload/1 (10).jpg'\n",
    "\n",
    "# Load and preprocess the image\n",
    "preprocessed_image = load_and_preprocess_image(new_image_path)\n",
    "\n",
    "# Predict the class of the image\n",
    "predictions = loaded_model.predict(preprocessed_image)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Load the image again in its original size for color extraction\n",
    "original_image = cv2.imread(new_image_path)\n",
    "original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Find the dominant color\n",
    "dominant_hex_color = find_dominant_color(original_image)\n",
    "color=get_color_name(dominant_hex_color)\n",
    "main_emotion = emotions[color]\n",
    "# Output\n",
    "# print(\"Predicted Class:\", predicted_class[0])\n",
    "print(\"Dominant Hex Color:\", dominant_hex_color)\n",
    "print(\"Predicted Color :\",color) \n",
    "print(\"Emotions:\",main_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6bd020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DC8F5DD260> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "#201e1a\n",
      "black\n",
      "['Powerful', 'sophisticated', 'edgy']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming the other functions (closest_color, get_color_name, find_dominant_color) are defined as before\n",
    "\n",
    "def process_video(video_path, model_path, img_size=(64, 64)):\n",
    "    # Load the trained model\n",
    "    loaded_model = load_model(model_path)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess the frame (resize, normalize)\n",
    "        frame_processed = cv2.resize(frame, img_size)\n",
    "        frame_processed = frame_processed / 255.0\n",
    "        frame_processed = np.expand_dims(frame_processed, axis=0)\n",
    "\n",
    "        # Predict the class for the current frame\n",
    "        predictions = loaded_model.predict(frame_processed)\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Convert frame to RGB and find the dominant color\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        dominant_hex_color = find_dominant_color(frame_rgb)\n",
    "        color = get_color_name(dominant_hex_color)\n",
    "        main_emotion = emotions[color]\n",
    "        return dominant_hex_color,color,main_emotion\n",
    "        # Process the results as needed (e.g., display, store, aggregate)\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "# Example usage\n",
    "video_path = 'upload/v2.mp4'\n",
    "model_path = 'model.h5'\n",
    "dominant_hex_color,color,main_emotion = process_video(video_path, model_path)\n",
    "print(dominant_hex_color)\n",
    "print(color)\n",
    "print(main_emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c0eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff29eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
